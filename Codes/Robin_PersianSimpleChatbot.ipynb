{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Table of contents**"
      ],
      "metadata": {
        "id": "LkCyvfs0ew8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Description](#updateTitle=true&folderId=11cmakAGPTH1PlyzXpA-EGRJVemg3GGFH&scrollTo=mfCvdfDjfCbK)\n",
        "\n",
        ">[Import Libraries](#updateTitle=true&folderId=11cmakAGPTH1PlyzXpA-EGRJVemg3GGFH&scrollTo=Zrj6oh8IWsPz)\n",
        "\n",
        ">[Import dataset](#updateTitle=true&folderId=11cmakAGPTH1PlyzXpA-EGRJVemg3GGFH&scrollTo=uvOTUnErYOxg)\n",
        "\n",
        ">[Data Preprocessing](#updateTitle=true&folderId=11cmakAGPTH1PlyzXpA-EGRJVemg3GGFH&scrollTo=UwsZI4ywYyR7)\n",
        "\n",
        ">[Create Deep Learning Model](#updateTitle=true&folderId=11cmakAGPTH1PlyzXpA-EGRJVemg3GGFH&scrollTo=2gnaMaU4apLl)\n",
        "\n",
        ">>[Batching](#updateTitle=true&folderId=11cmakAGPTH1PlyzXpA-EGRJVemg3GGFH&scrollTo=N5og9VUdbBid)\n",
        "\n",
        ">>[Setting Hyper Parameters, Loss Functions, and Optimizer](#updateTitle=true&folderId=11cmakAGPTH1PlyzXpA-EGRJVemg3GGFH&scrollTo=HvZO6C-lbHdm)\n",
        "\n",
        ">>[Train the model](#updateTitle=true&folderId=11cmakAGPTH1PlyzXpA-EGRJVemg3GGFH&scrollTo=jtQ6hHwmbcgX)\n",
        "\n",
        ">[Design User Interface](#updateTitle=true&folderId=11cmakAGPTH1PlyzXpA-EGRJVemg3GGFH&scrollTo=2Zf1QB5hcEft)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "eeuT14OceuHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description"
      ],
      "metadata": {
        "id": "mfCvdfDjfCbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Robin is my \"final project\" as a bachelor of computer engineering. In this project, I first created a dataset in Persian. This dataset includes three main parts, TAG, PATTERNS, and RESPONSES. The approach is that I categorized some of the possible questions. For example, the input from the user can be 1-Hi, 2_Hello, 3_How are you, 4_How are you doing, etc. All of these questions belong to one category called GREETING. So their tag would be greeting. On the other hand, each tag provides some pre-specified answers(called RESPONSES in the dataset) hence after Robin understands the tag based on user input, it will refer to its database and choose one of the pre-specified answers randomly. In fact, Robin learns the tags based on user input."
      ],
      "metadata": {
        "id": "EKk_LGDkfG2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "Zrj6oh8IWsPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: If you want to run this code, you first need to install the Hazm(a library for processing Persian words).**"
      ],
      "metadata": {
        "id": "hUBl-TLBXSE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8PUmSA0Wzup",
        "outputId": "b1123015-95e9-4434-d96e-9fc765ff4729"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.10)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G_ybVjZcWdeA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import json\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import tkinter as tk\n",
        "from tkinter import scrolledtext\n",
        "import hazm\n",
        "import torch\n",
        "import random\n",
        "from PIL import Image, ImageTk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset"
      ],
      "metadata": {
        "id": "uvOTUnErYOxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r'data.json path', encoding='utf-8', errors='ignore') as input_data:\n",
        "    data = json.load(input_data)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC7Y-x-XYTDc",
        "outputId": "34784268-eb41-46d4-e70b-507045ba4390"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': [{'tag': 'greeting',\n",
              "   'patterns': ['سلام',\n",
              "    'خوبی',\n",
              "    'درود بر تو',\n",
              "    'حالت چطوره',\n",
              "    'احوالت چطوره',\n",
              "    'سلام بر تو',\n",
              "    'سلام علیکم',\n",
              "    'سلام روز بخیر'],\n",
              "   'responses': ['سلام بر شما، در خدمتم.',\n",
              "    'سلام بر شما، بفرمایید.',\n",
              "    'سلام، وقت شما بخیر. بفرمایید.',\n",
              "    'سلام، امیدوارم حال شما خوب باشه. بفرمایید.',\n",
              "    'درود برای خدمت رسانی آماده ام',\n",
              "    'درود دوست عزیزم، جانم']},\n",
              "  {'tag': 'goodbye',\n",
              "   'patterns': ['خداحافظ.',\n",
              "    'بدرود.',\n",
              "    'در پناه حق.',\n",
              "    'به امید دیدار',\n",
              "    'خدا نگه دار',\n",
              "    'فعلا خدا نگهدار'],\n",
              "   'responses': ['در پناه حق.',\n",
              "    'مواظب خودت باش.',\n",
              "    'به امید دیدار.',\n",
              "    'مواظب باش',\n",
              "    'خدا حافظ']},\n",
              "  {'tag': 'funny',\n",
              "   'patterns': ['یه جوک برام تعریف کن.',\n",
              "    'یک جوک خنده دار بگو.',\n",
              "    'یه لطیفه تعریف کن.',\n",
              "    'یه چیز خنده دار بگو.',\n",
              "    'میتونی یه جوک برام تعریف کنی',\n",
              "    'چه چیزای خنده داری بلدی',\n",
              "    'بهم لطیفه بگو',\n",
              "    'بهم جوک بگو'],\n",
              "   'responses': ['با این قیمت طلا دیگه نمیشه حلقه خرید. باید یه اسپری قرمز بگیریم رو نامزدمون ضربدر بزنیم نبرنش.',\n",
              "    'برای شنیدن جوک عدد 1 رو به شماره دلخواه پیامک کن',\n",
              "    'ایران بهترین کشور جهانه',\n",
              "    'دانشگاه آزاد اسلامی تنها جاییه که نه دانشگاه عه، نه آزاده و نه اسلامیه',\n",
              "    'آقاهه زنگ زده بانک میگه اگه من وام ماشین رو ندم بانک ماشین رو از من میگیره!!! بانک: بله آقا حتما این کار رو خواهیم کرد. آقاهه: در مورد وام ازدواج هم همین شرایط رو دارین؟']},\n",
              "  {'tag': 'aboutbot',\n",
              "   'patterns': ['درباره خودت بگو',\n",
              "    'خودت رو معرف کن',\n",
              "    'خودت رو چطور معرفی میکنی',\n",
              "    'چرا ساختنت',\n",
              "    'برای چه هدفی ساخته شدی',\n",
              "    'هدف از ساخت تو چی بوده'],\n",
              "   'responses': ['من یه ربات چت هستم که سعی میکنم به آدم ها درباره سوال هاشون درست ترین پاسخ رو بدم',\n",
              "    'یه چتبات ام که توسط علیرضا هادیپور دانشجوی دانشگاه گیلان ایجاد شدم و قصدم کمک به آدم هاست',\n",
              "    'اسمم رابین هست و توسط علیرضا ایجاد شدم، اینجام که به سوالاتتون در حد فهم خودم جواب بدم',\n",
              "    'اسمم رابینه و اینجام تا به سوالاتتون پاسخ بدم',\n",
              "    'من به عنوان پروژه پایانی کارشناسی علیرضا بوجود اومدم و اینجام تا کمکت کنم، راستی اسمم رابینه',\n",
              "    'رابین هستم، دوست علیرضا، یه چتبات که آرزوش اینه ی روزی به چت جیپیتی برسه :)))']},\n",
              "  {'tag': 'aboutcreator',\n",
              "   'patterns': ['چه کسی ساختت',\n",
              "    'سازندت کیه',\n",
              "    'کی ساختت',\n",
              "    'به چه هدفی ایجاد شدی',\n",
              "    'برای چه ساخته شدی',\n",
              "    'نویسندت کیه',\n",
              "    'کدت رو کی زده',\n",
              "    'اسم نویسندت چیه',\n",
              "    'کی ساختت',\n",
              "    'تولید کنندت کیه'],\n",
              "   'responses': ['سازندم علیرضا دانشجوی کامپیوتره، ایناهاش: https://www.linkedin.com/in/alireza-hadipoor/',\n",
              "    ' علیرضا برای پروژه پایانی ایجادم کرده از این لینک میتونی لینکداین عش رو ببینی https://www.linkedin.com/in/alireza-hadipoor/',\n",
              "    'علیرضا، ایناهاش https://www.linkedin.com/in/alireza-hadipoor/']},\n",
              "  {'tag': 'aboutai',\n",
              "   'patterns': ['هوش مصنوعی چیه',\n",
              "    'درباره هوش مصنوعی برام بگو',\n",
              "    'درباره هوش مصنوعی چی میدونی',\n",
              "    'هوش مصنوعی',\n",
              "    'هوش مصنوعی چیست'],\n",
              "   'responses': ['هوشی است که توسط ماشین\\u200cها ظهور پیدا می\\u200cکند، در مقابل هوش طبیعی که توسط جانوران شامل انسان\\u200cها نمایش می\\u200cیابد. اما پیش از هرچیز باید این موضوع را دانست که کلمه هوش، نشان دهنده امکان استدلال است و اینکه آیا هوش مصنوعی می\\u200cتواند به توانایی استدلال دست یابد یا خیر، خود موضوع اختلاف محققان است. کتاب\\u200cهای AI پیشرو، این شاخه را به عنوان شاخه مطالعه بر روی «عوامل هوشمند» تعریف می\\u200cکنند: هر سامانه\\u200cای که محیط خود را درک کرده و کنش\\u200cهایی را انجام می\\u200cدهد که شانسش را در دستیابی به اهدافش بیشینه می\\u200cسازد برخی از منابع شناخته شده از اصطلاح «هوش مصنوعی» جهت توصیف ماشینی استفاده می\\u200cکنند که عملکردهای «شناختی» را از روی ذهن انسان\\u200cها تقلید می\\u200cکنند، همچون «یادگیری» و «حل مسئله»، با این حال این تعریف توسط محققان اصلی در زمینه AI رد شده\\u200cاست.']},\n",
              "  {'tag': 'bestproff',\n",
              "   'patterns': ['بهترین استاد های دانشکده فنی رو بگو',\n",
              "    'بهترین استاد فنی کیه',\n",
              "    'استاد مورد علاقت کیه'],\n",
              "   'responses': ['قطعا استاد احمدیفر یکی از بهتریناست',\n",
              "    'استاد احمدیفر، دکتر احمدیفر، مهندس احمدیفر',\n",
              "    'استاد احمدیفر از گروه کامپیوتر دانشگاه گیلان',\n",
              "    'معلومه استاد احمدیفر']},\n",
              "  {'tag': 'bestcourse',\n",
              "   'patterns': ['بهترین درس های مهندسی کامپیوتر چیا هستند',\n",
              "    'مهم ترین درس های مهندسی کامپیوتر رو نام ببر',\n",
              "    'درس های خوب کامپیوتر چی هستند',\n",
              "    'دروس مهم کامپیوتر',\n",
              "    'دروس مهم مهندسی کامپیوتر',\n",
              "    'یه مهندس کامپیوتر خوب باید چه درس هاییش خوب باشه',\n",
              "    'بهترین درس های مهندسی کامپیوتر چیه',\n",
              "    'مهندسی کامپیوتر میخونم، کدوم درس هارو باید خوب بلد باشم'],\n",
              "   'responses': ['همه درس ها مهمن و باید خوب بخونیدشون، ولی ساختمان و الگوریتمو مشتی وار بخونید',\n",
              "    'ساختمان داده و طراحی الگوریتم',\n",
              "    'بستگی داره کدوم زیرشاخرو دوست داشته باشی ولی در کل هوش و ساختمان و الگوریتم رو خوب بخون',\n",
              "    'بجز عمومی ها همرو بخون',\n",
              "    'ساختمان داده و طراحی الگوریتم تقریبا تو هر زیر شاخه ای مهم ترینان',\n",
              "    'بنظرم همرو بخون، ساختمانو بیشتر بخون']},\n",
              "  {'tag': 'bestuniversity',\n",
              "   'patterns': ['بهترین دانشگاه های ایران رو نام ببر',\n",
              "    'اسم چند تا از دانشگاه های ایران که خیلی قوی هستند رو بگو',\n",
              "    'از نظرت کدوم دانشگاه هارو انتخاب کنم',\n",
              "    'بهترین دانشگاه ها از نظرت کدومان',\n",
              "    'اسم چند تا دانشگاه رو برام بگو',\n",
              "    'دانشگاه های برتر ایران',\n",
              "    'دانشگاه های تاپ ایران'],\n",
              "   'responses': ['تو ایران: شریف، تهران، امیرکبیر',\n",
              "    'ایرانی : شریف و تهران و بهشتی و امیر کبیر و خارجی: آکسفورد و کمبریج و استنفورد',\n",
              "    'دانشگاه خوب زیاده ولی هیچجا گیلان نمیشه']},\n",
              "  {'tag': 'cold_diseas',\n",
              "   'patterns': ['درباره سرماخوردگی چی میدونی',\n",
              "    'درباره سرماخوردگی چه اطلاعاتی دارید',\n",
              "    'آنفولانزا',\n",
              "    'سرماخوردگی چیست',\n",
              "    'سرماخوردگی چیه',\n",
              "    'ویروس سرما خوردگی',\n",
              "    'سرماخوردگی'],\n",
              "   'responses': ['سرماخوردگی به عنوان یک بیماری عفونی شایع در بخش\\u200cهای بالایی دستگاه تنفسی مانند بینی و گلو توصیف می\\u200cشود. علل سرماخوردگی ممکن است شامل عفونت ویروسی، عفونت باکتریایی یا هر دو باشد. ویروس\\u200cهای سرماخوردگی معمولاً به عنوان ویروس\\u200cهای رینوویروس، کروناویروس، ویروس\\u200cهای آدنوویروس و ویروس\\u200cهای آنفلوانزا مشخص می\\u200cشوند.']},\n",
              "  {'tag': 'datastructer',\n",
              "   'patterns': ['درس ساختمان داده چیه',\n",
              "    'ساختمان داده چیست',\n",
              "    'ساختمان های داده',\n",
              "    'ساختمان داده رو تعریف کن'],\n",
              "   'responses': ['مجموعه\\u200cای از مقادیر داده و روابط بین آنها است. ساختمان داده به برنامه\\u200cها اجازه می\\u200cدهد تا داده\\u200cها را به\\u200cطور موثر ذخیره و پردازش کنند. ساختمان داده یک زبان برنامه\\u200cنویسی مانند C، سی پلاس پلاس، Java و غیره نیست بلکه مجموعه\\u200cای از الگوریتم\\u200cها است که می\\u200cتواند در هر زبان برنامه نویسی برای سازماندهی داده\\u200cها در حافظه استفاده شود. ساختمان داده\\u200cهای مختلفی وجود دارد که هر کدام مزایا و معایب خاص خود را دارند، برخی از رایج\\u200cترین ساختمان داده\\u200cها عبارتند از: آرایه، لیست\\u200cها، درخت\\u200cها و گراف.']},\n",
              "  {'tag': 'ghahve',\n",
              "   'patterns': ['قهوه چیست',\n",
              "    'درباره قهوه چه میدانی',\n",
              "    'قهوه',\n",
              "    'قهوه رو توضیح بده',\n",
              "    'درباره قهوه برام توضیح بده',\n",
              "    'درباره قهوه بهم بگو'],\n",
              "   'responses': ['قَهوِه گونه\\u200cای نوشیدنی رایج است که از دانه\\u200cهای بوداده و آسیاب\\u200cشدهٔ گیاه قهوه به\\u200cدست می\\u200cآید. قهوه کمی اسیدی است و به\\u200cعلت داشتن کافئین بالا، یک مادهٔ محرک است. این نوشیدنی، محبوب\\u200cترین نوشیدنی گرم در جهان است.']},\n",
              "  {'tag': 'software_engineering',\n",
              "   'patterns': ['مهندسی نرم افزار چیست',\n",
              "    'مهندسی نرم افزار',\n",
              "    'نرم افزار',\n",
              "    'درباره مهندسی نرم افزار برام توضیح بده',\n",
              "    'درباره مهندسی نرم افزار بهم بگو'],\n",
              "   'responses': ['یعنی استفاده از اصول مهندسی بجا و مناسب برای تولید و ارائه محصول نرم\\u200cافزاری با کیفیت که قابل اطمینان و با صرفه بوده و بر روی ماشین\\u200cهای واقعی به\\u200cطور کارآمدی عمل کند',\n",
              "    'مهندسی نرم\\u200cافزار یک روش سیستماتیک، منظم و دقیق برای ساخت و ارائه محصولی نرم\\u200cافزاری با کیفیت است']},\n",
              "  {'tag': 'programminglanguages',\n",
              "   'patterns': ['بهترین زبانهای برنامه نویسی کدام هستند',\n",
              "    'بهترین زبانهای برنامه نویسی را نام ببر',\n",
              "    'بهترین زبانهای برنامه نویسی',\n",
              "    'زبانهای برنامه نویسی مهم'],\n",
              "   'responses': ['پایتون، جاوا و جاوا اسکریپت بهترین زبانهای برنامه نویسی هستند',\n",
              "    'بنا به کاربرد میتونید زبان برنامه نویسی مناسب خودتون رو انتخاب کنید، اما پایتون و جاوا بسیار معروف هستند']},\n",
              "  {'tag': 'iran',\n",
              "   'patterns': ['ایران کجاست',\n",
              "    'درباره ایران بهم بگو',\n",
              "    'ابران چجور جاییه',\n",
              "    'ایران'],\n",
              "   'responses': ['ایران با نام رسمی جمهوری اسلامی ایران، کشوری در غرب آسیا است. این کشور از غرب با ترکیه و عراق، از شمال غربی با ارمنستان و جمهوری آذربایجان، از شمال با دریای خزر و ترکمنستان، از شرق با افغانستان و پاکستان، و از جنوب با دریای عمان و خلیج فارس هم\\u200cمرز است. ایران با دارا بودن پهناوری به مساحت ۱٫۶۴ میلیون کیلومتر مربع، هفدهمین کشور پهناور جهان است، و با جمعیت تخمینی ۸۹٫۴ میلیون نفری، در رتبهٔ هفدهم کشورها با بیشترین جمعیت، قرار می\\u200cگیرد ایران در تقسیمات کشوری به ۳۱ استان، تقسیم می\\u200cشود. زبان رسمی کشور ایران، فارسی است. پرجمعیت\\u200cترین شهرهای ایران به\\u200cترتیب تهران (پایتخت)، مشهد، اصفهان، کرج و شیراز هستند.']},\n",
              "  {'tag': 'tarif',\n",
              "   'patterns': ['خیلی خوب حرف میزنی',\n",
              "    'خیلی خوبی',\n",
              "    'چقدر خوبی تو',\n",
              "    'خیلی باحالی',\n",
              "    'چه ربات باحالی هستی',\n",
              "    'آفرین بهت',\n",
              "    'به به',\n",
              "    'دمتگرم'],\n",
              "   'responses': ['خیلی ممنونم، نظر لطفتونه (^.^)']},\n",
              "  {'tag': 'limit',\n",
              "   'patterns': ['تا چه حد بلدی',\n",
              "    'چه کارهای میتونی انجام بدی',\n",
              "    'چقدر اطلاعات داری',\n",
              "    'چه چیزهایی بلدی',\n",
              "    'دانشت تا چه حد هست',\n",
              "    'چه اطلاعاتی داری'],\n",
              "   'responses': ['من روی دیتای خیلی کم و روی مدل بسسار کوچکی ترین شدم برای همین اطلاعاتم محدود به چیزایی هست که برام تعریف شده، همچنین خطاهایی هم دارم']},\n",
              "  {'tag': 'thanks',\n",
              "   'patterns': ['ممنونم',\n",
              "    'خیلی لطف کردی',\n",
              "    'متشکرم',\n",
              "    'خیلی ممنونم',\n",
              "    'با تشکر از شما',\n",
              "    'متشکرم',\n",
              "    'خیلی متشکرم'],\n",
              "   'responses': ['خواهش میکنم :))))', 'نظر لطفته']},\n",
              "  {'tag': 'ok',\n",
              "   'patterns': ['باشه', 'اوکی', 'اها حله', 'اوکیه', 'خب حالا'],\n",
              "   'responses': ['اوکی :)']},\n",
              "  {'tag': 'chatgpt',\n",
              "   'patterns': ['چت جی پی تی چیه',\n",
              "    'چت جیپیتی چیه',\n",
              "    'درباره چت جیپیتی برام بگو',\n",
              "    'درباره چت جی پی تی برام بگو',\n",
              "    'درباره چت جیپیتی چی میدونی',\n",
              "    'درباره چت جی پی تی چی میدونی',\n",
              "    'چت جیپیتی',\n",
              "    'چت جی پی تی',\n",
              "    'چت جیپیتی چیست',\n",
              "    'چت جی پی تی چیست'],\n",
              "   'responses': ['یک بات مکالمه است که توسط شرکت اوپن ای\\u200cآی توسعه یافته\\u200cاست؛ گروه اوپن اِی\\u200cآی برعکس نسخه\\u200cهای قبلی که با نام پردازش زبانی منتشر شده بود این بار ابزاری با نام بات مکالمه منتشر کرد. چت\\u200cجی\\u200cپی\\u200cتی بر روی خانواده مدل زبانی جی\\u200cپی\\u200cتی ۳٫۵ و جی\\u200cپی\\u200cتی ۴ اوپن ای\\u200cآی ساخته شده و با تکنیک\\u200cهای یادگیری نظارت شده و تقویتی به\\u200cخوبی تنظیم شده\\u200cاست.']},\n",
              "  {'tag': 'ml',\n",
              "   'patterns': ['یادگیری ماشین',\n",
              "    'ماشین لرنینگ',\n",
              "    'ماشین لرنینگ چیست',\n",
              "    'یادگیری ماشین چیست',\n",
              "    'ماشین لرنینگ چیه',\n",
              "    'درباره ماشین لرنینگ چی میدونی',\n",
              "    'درباره یادگیری ماشین چی میدونی'],\n",
              "   'responses': ['مطالعهٔ الگوریتم\\u200cها و مدل\\u200cهای آماری مورد استفادهٔ سیستم\\u200cهای کامپیوتری است که به\\u200cجای استفاده از دستورالعمل\\u200cهای واضح، از الگوها و استنباط برای انجام وظایف استفاده می\\u200cکنند. یادگیری ماشینی علمی است که باعث می\\u200cشود رایانه\\u200cها بدون نیاز به یک برنامه صریح در مورد یک موضوع خاص یاد بگیرند. به عنوان زیر مجموعه\\u200cای از هوش مصنوعی، الگوریتم\\u200cهای یادگیری ماشینی یک مدل ریاضی بر اساس داده\\u200cهای نمونه یا داده\\u200cهای آموزش به منظور پیش\\u200cبینی یا تصمیم\\u200cگیری بدون برنامه\\u200cریزی آشکار، ایجاد می\\u200cکنند.']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "UwsZI4ywYyR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to preprocess the data and separate them into X_train and y_train"
      ],
      "metadata": {
        "id": "OkhFEk46jhe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = hazm.stemmer.Stemmer()\n",
        "stemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXETxDPdYYcD",
        "outputId": "93b8e0e7-96c4-49d8-d32b-21c82388c751"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<hazm.stemmer.Stemmer at 0x7a88dcd32260>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = [\"؟\", \"!\", \".\", \"،\", \":\", \"؛\", \"?\", \"/\", \"`\", \"~\", '', ' ']\n",
        "tags = []\n",
        "words = []\n",
        "pattern_tags = []"
      ],
      "metadata": {
        "id": "QBiK6PYgY3Ds"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for instance in data[\"data\"]:\n",
        "    tag = instance[\"tag\"]\n",
        "    tags.append(tag)\n",
        "\n",
        "    for pattern in instance[\"patterns\"]:\n",
        "        tokenized_pattern = hazm.word_tokenize(pattern)\n",
        "        stemmed_pattern = [stemmer.stem(word) for word in tokenized_pattern if word not in stop_words]\n",
        "        words.extend(stemmed_pattern)\n",
        "        pattern_tags.append((stemmed_pattern, tag))"
      ],
      "metadata": {
        "id": "R1nvxJe4ZQxY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = sorted(set(words))\n",
        "words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4EaejlSZV3H",
        "outputId": "ec870c4c-b4f2-4739-e9e2-6057504a78be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'آفرین', 'آنفولانزا', 'ا', 'ابر', 'احوال', 'از', 'اس', 'استاد', 'اطلاع']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding(source, words):\n",
        "    src = [stemmer.stem(word) for word in source]\n",
        "    one_hot_encoding = np.zeros(len(words), dtype='float32')\n",
        "\n",
        "    for index, word in enumerate(words):\n",
        "        if word in src:\n",
        "            one_hot_encoding[index]=1\n",
        "\n",
        "    return one_hot_encoding"
      ],
      "metadata": {
        "id": "WoWyjK_WZXGi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for X, y in pattern_tags:\n",
        "    X = encoding(X, words)\n",
        "    X_train.append(X)\n",
        "\n",
        "    y = tags.index(y)\n",
        "    y_train.append(y)\n",
        "\n",
        "X_train = torch.from_numpy(np.array(X_train)).to(dtype=torch.float)\n",
        "y_train = torch.from_numpy(np.array(y_train)).to(dtype=torch.long)"
      ],
      "metadata": {
        "id": "9_BFP1pgaFoO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Deep Learning Model"
      ],
      "metadata": {
        "id": "2gnaMaU4apLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The deep learning model has 4 layers:\n",
        "\n",
        "*First Layer*: input: len(words) units,      output: 120 units\n",
        "\n",
        "*Second Layer*: input: 120 units,            output: 64 units\n",
        "\n",
        "*Third Layer*: input: 64 units,              output: 36 units\n",
        "\n",
        "*Fourth Layer*: input: 36 units,             output: len(tags) units"
      ],
      "metadata": {
        "id": "QnUBThskj8pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBot(nn.Module):\n",
        "    def __init__(self, inp_size, out_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(inp_size,120)\n",
        "        self.layer2 = nn.Linear(120,64)\n",
        "        self.layer3 = nn.Linear(64,36)\n",
        "        self.layer4 = nn.Linear(36, out_size)\n",
        "        self.Relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.layer1(self.Relu(x))\n",
        "        output = self.layer2(self.Relu(output))\n",
        "        output = self.layer3(self.Relu(output))\n",
        "        output = self.layer4(self.Relu(output))\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "Nd0Wd2GEasNv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batching"
      ],
      "metadata": {
        "id": "N5og9VUdbBid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to separate the dataset into batches because we are using a mini-batch approach. This approach will update W's and B's at each batch's end. So we use this to update W's and B's rapidly."
      ],
      "metadata": {
        "id": "Ql-sC2TelWq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBotDataset(Dataset):\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.n_samples = len(X_train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X_train[idx], self.y_train[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "def data_loader(X_train, y_train, batch_size):\n",
        "    dataset = ChatBotDataset(X_train, y_train)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader"
      ],
      "metadata": {
        "id": "tFKzDPOQa3ox"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Hyper Parameters, Loss Functions, and Optimizer"
      ],
      "metadata": {
        "id": "HvZO6C-lbHdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp_size = X_train.shape[1]\n",
        "output_size = len(tags)\n",
        "model = ChatBot(inp_size, output_size)\n",
        "batch_size = 10\n",
        "alpha = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), alpha)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "4ABBuOwVbSda"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "jtQ6hHwmbcgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, loss_fn, optimizer, epochs):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for X, y in dataloader:\n",
        "\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0:\n",
        "            print(f\"epoch: {epoch+1}/{epochs}   loss: {loss.item():.4f}\")\n",
        "\n",
        "train_loader = data_loader(X_train, y_train, batch_size)\n",
        "train(model=model, dataloader=train_loader, optimizer=optimizer, loss_fn=loss_func, epochs=320)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZhamGYubcPT",
        "outputId": "2fba9751-d402-4785-d26f-312a3cdfea7d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 50/320   loss: 2.4017\n",
            "epoch: 100/320   loss: 0.3113\n",
            "epoch: 150/320   loss: 0.1585\n",
            "epoch: 200/320   loss: 0.0630\n",
            "epoch: 250/320   loss: 0.0272\n",
            "epoch: 300/320   loss: 0.0045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the loss is reduced after each epoch ends.\n",
        "\n"
      ],
      "metadata": {
        "id": "YU1YlpTBmLSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: Normally we use the TEST set to evaluate the final performance of the model, but in this case, due to the small data set, I did not.**"
      ],
      "metadata": {
        "id": "cNpe23RtmsE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design User Interface"
      ],
      "metadata": {
        "id": "2Zf1QB5hcEft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: If you're using Colab Notebook, this part of the project(user interface) will not work for you. Because Colab doesn't support the Tkinter library. So I recommend you download and run it in your code editor(like VS Code). There is file named main.py that you can download**"
      ],
      "metadata": {
        "id": "TSV7Tem8nbiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exit_prog(event=None):\n",
        "    window.destroy()\n",
        "\n",
        "def send(event=None):\n",
        "    user_input = user_text.get()\n",
        "    user_text.set('')\n",
        "    if user_input == 'تمام':\n",
        "        window.quit()\n",
        "\n",
        "    chat_log.config(state=tk.NORMAL)\n",
        "    chat_log.insert(tk.END, \"You: \" + user_input + '\\n')\n",
        "\n",
        "    tokenized = hazm.word_tokenize(user_input)\n",
        "    embed = encoding(tokenized, words)\n",
        "    embed = embed.reshape(1, len(embed))\n",
        "    X = torch.from_numpy(embed).to(dtype=torch.float)\n",
        "\n",
        "    output = model(X)\n",
        "    _, y_pred = torch.max(output, dim=1)\n",
        "    find_tag = tags[y_pred.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][y_pred.item()]\n",
        "\n",
        "    response = \"\"\n",
        "    if prob > 0.55:\n",
        "        for instance in data[\"data\"]:\n",
        "            if instance[\"tag\"] == find_tag:\n",
        "                response = random.choice(instance[\"responses\"])\n",
        "    else:\n",
        "        response = \"منظورتان را درک نکردم لطفا به شیوه ی دیگری بیان کنید\"\n",
        "\n",
        "    chat_log.insert(tk.END, \"Robin: \" + response + '\\n\\n')\n",
        "    chat_log.config(state=tk.DISABLED)\n",
        "    chat_log.yview(tk.END)\n",
        "\n",
        "window = tk.Tk()\n",
        "window.title(\"Robin Chatbot: Final Bachelor Project, ALIREZA HADIPOOR\")\n",
        "window.configure(background='light blue')\n",
        "\n",
        "image = Image.open(\"image.png\")\n",
        "bg_image = ImageTk.PhotoImage(image)\n",
        "background_label = tk.Label(window, image=bg_image)\n",
        "background_label.place(x=0, y=0, relwidth=1, relheight=1)\n",
        "\n",
        "user_text = tk.StringVar()\n",
        "user_entry = tk.Entry(window, textvariable=user_text)\n",
        "user_entry.pack(padx=20, pady=20)\n",
        "\n",
        "chat_log = scrolledtext.ScrolledText(window, state=tk.DISABLED, wrap=tk.WORD, bg='light blue')\n",
        "chat_log.pack(pady=10)\n",
        "\n",
        "button_frame = tk.Frame(window, bg='gray')\n",
        "button_frame.pack(side=tk.BOTTOM, fill=tk.X, pady=55)\n",
        "\n",
        "send_button = tk.Button(button_frame, text=\"Send\", command=send, height=2, width=15, bg=\"light blue\", fg='black')\n",
        "send_button.pack(side=tk.LEFT, padx=250, pady=5)\n",
        "\n",
        "exit_button = tk.Button(button_frame, text='Exit', command=exit_prog, height=2, width=15, bg=\"light blue\", fg='black')\n",
        "exit_button.pack(side=tk.RIGHT, padx=250, pady=5)\n",
        "\n",
        "window.state('zoomed')\n",
        "window.bind('<Return>', send)\n",
        "window.bind('<Escape>', exit_prog)\n",
        "window.mainloop()"
      ],
      "metadata": {
        "id": "uE17lzM1cIAj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}